{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The shape of a Convolutional Neural Network (CNN) typically has a length of 4: $len([α_0,α_1,α_2,α_3]) = 4$.  \n",
    "This means, we have a rank 4 tensor with 4 axes and the value at each index gives us the *length* of this corresponding axis: $[Length_0,Length_1,Length_2,Length_3]$\n",
    "\n",
    "• For images the raw data comes in the form of pixels represented by a number, and layed out in 2 dimensions (height $x$ width) \n",
    "> The image height and width are represented in the **last two axes.**: $[α_0,α_1,height,width]$ \n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/height_width.JPG) \n",
    "\n",
    "</div>\n",
    "\n",
    "> The next axis represents the **Color Channels**: $[α_0,color-channels,height,width]$ \n",
    "\n",
    "3 Values for (R,G,B):\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/rgb.jpg) \n",
    "\n",
    "</div>\n",
    "\n",
    "Or 1 for grayscale images:\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/grayscale.JPG) \n",
    "\n",
    "</div> \n",
    "\n",
    "> Note the color interpretation only applies to the *input tensor*.\n",
    "> The interpretation of this axis $(a_1)$, changes after the tensor passes through a convolutional layer.\n",
    "\n",
    "So far, we have used the last three axes of the tensor $(α_1,α_2,α_3$) to represent an image as a tensor.\n",
    "> So to define a pixel we need: Color channel, Height, Width: \n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/pixel.JPG) \n",
    "\n",
    "</div> \n",
    "\n",
    "This brings us to $α_0$ : **The batch size**.\n",
    "In Neural Networks, we usually work with batches of samples, instead of a single sample.\n",
    ">So the length of axis $α_0$ tells us how many samples are in our batch.\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/batches.JPG) \n",
    "\n",
    "</div> \n",
    "\n",
    "> In conclusion: $[batch-size,color-channel,height,width]$\n",
    "> This allows us to see an entire batch of images represented by a single rank 4 tensor!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For example lets say we have the following shape: [3,1,28,28]\n",
    "This means that the images are 28 x 28 pixels, 1 color channel means grayscale, and we have a batch of 3 images.\n",
    "\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/batch_of_3.JPG) \n",
    "\n",
    "</div>\n",
    "\n",
    "This is a rank 4 neural network that will flow through our neural network.   \n",
    "\n",
    ">**So, now we can essentially navigate to a *specific pixel* in a *specific color channel* of a *specific image in the batch*!!**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### After the tensor is transformed by the convolutional layer ####\n",
    "Lets take for example a grayscale image, 28 x 28 px. This gives us the following tensor:\n",
    "$[1,1,28,28]$.\n",
    "When this image passes through the convolutional neural network, the *shape of the tensor* *(and the underlying data!)* will be changed by the convolution operation. The  convolution can change the height and width of the image as well as the number of color channels (based on the number of filters used in the layer.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### What happens to the color channel axis.\n",
    "Suppose we have three convolutional filters. This means we will have three channel outputs from the convolutional layer.\n",
    "Each of these filters convolve the input color channel producing three output channels.\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![image_a](Img/convolutions.JPG) \n",
    "\n",
    "</div>\n",
    "\n",
    "> Now out tensor has *changed* from $[1,1,28,28]$ to $[1,3,28,28]$. This shows us that now, we don't have color channels anymore (after the convolution), but *modified color channels* and the number $α_1$ (3 in this case), shows the amount of modification that are output. \n",
    "> These channels are called **feature maps**. We call them feature maps because the output from each convolution, produces/focuses on one feature, like edges for example (edge detection)."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "name": "python",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
