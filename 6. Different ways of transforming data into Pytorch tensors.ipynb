{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Creating Tensors."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets create a numpy array, and turn into tensors using the four different options:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[1 2 3] <class 'numpy.ndarray'>\n"
     ]
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "print(data,type(data))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([1., 2., 3.])\n",
      "t2: tensor([1, 2, 3], dtype=torch.int32)\n",
      "t3: tensor([1, 2, 3], dtype=torch.int32)\n",
      "t4: tensor([1, 2, 3], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "t1 = torch.Tensor(data) # Class constructor.\n",
    "t2 = torch.tensor(data) # Factory function, allows more dynamic object creation.\n",
    "t3 = torch.as_tensor(data) # Factory function.\n",
    "t4 = torch.from_numpy(data) # Factory function.\n",
    "print(f't1: {t1}')\n",
    "print(f't2: {t2}')\n",
    "print(f't3: {t3}')\n",
    "print(f't4: {t4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Check coresponding datatype:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: torch.float32\n",
      "t2: torch.int32\n",
      "t3: torch.int32\n",
      "t4: torch.int32\n"
     ]
    }
   ],
   "source": [
    "print(f't1: {t1.dtype}')\n",
    "print(f't2: {t2.dtype}')\n",
    "print(f't3: {t3.dtype}')\n",
    "print(f't4: {t4.dtype}')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The **torch.Tensor()** returns float32 because the constructor uses the global default data type."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.float32"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.get_default_dtype()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The other factory functions *infer* the data type.\n",
    "They choose the datatype based on incoming data. (Type inference)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Integer coming in - integer coming out.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1, 2, 3], dtype=torch.int32)"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([1,2,3]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "*Floating-point coming in - floating point coming out*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([1.,2.,3.]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can also set the dtype explicitly:  \n",
    "(*Notice that, although we pass ints, the dtype turns to float because we explicitly passed it.*)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([1., 2., 3.], dtype=torch.float64)"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.tensor(np.array([1,2,3]),dtype=torch.float64)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> All of the above has been what we can *visually* detect, in terms of differences, just by inspecting output. There are more differences however, behind the scenes."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Memory: Sharing vs Copying"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lets start with the initial np.array:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([1, 2, 3])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = np.array([1,2,3])\n",
    "data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "t1 = torch.Tensor(data) \n",
    "t2 = torch.tensor(data) \n",
    "t3 = torch.as_tensor(data)\n",
    "t4 = torch.from_numpy(data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we leave the tensors alone, but we modify the data we used (*np.array([1,2,3])*) when creating the numpy array that we in turn used, when we created our tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [],
   "source": [
    "data[0] = 0\n",
    "data[1] = 0\n",
    "data[2] = 0"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We modified the contents of the initial list, time to see what happens to the tensors."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t1: tensor([1., 2., 3.])\n",
      "t2: tensor([1, 2, 3], dtype=torch.int32)\n",
      "t3: tensor([0, 0, 0], dtype=torch.int32)\n",
      "t4: tensor([0, 0, 0], dtype=torch.int32)\n"
     ]
    }
   ],
   "source": [
    "print(f't1: {t1}')\n",
    "print(f't2: {t2}')\n",
    "print(f't3: {t3}')\n",
    "print(f't4: {t4}')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "> The **torch.Tensor(**) and **torch.tensor()** *haven't changed*. This means that they *still contain the data from the original [1,2,3] numpy array*. Changing the array did not affect the t1, t2 tensor data.  \n",
    "> \n",
    "> The **torch.as_tensor()** and **torch.from_numpy()**, *contain the same data that is now in the array, after the change*\n",
    ">\n",
    "> The reason: t1 and t2 create an additional copy of the input data in memory, while the t3 and t4 tensors share data in memory with the numpy array.\n",
    "<div style=\"text-align: center;\">\n",
    "\n",
    "![Share - Copy](Img\\Share_copy_data.JPG)\n",
    "\n",
    "</div>\n",
    "\n",
    "This sharing just means that the actual data in memory exists in a single place. As a result, any changes that occur in the underlying data will be reflected in both objects, the **torch.Tensor** and the **numpy.ndarray**.\n",
    "\n",
    "> Moving between numpy arrays and pytorch tensors can be very fast because the data is shared and not copied behind the scenes when creating new pytorch tensors.*When we say the data is shared we mean that the data exists in a **single place**.* As a result any changes that occur in the underlying data will be reflected in both objects; the array and the tensor."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### ***Best options for creating tensors in Pytorch:***\n",
    "\n",
    " Given all of these details, these two are the best options:\n",
    "\n",
    "    torch.tensor()\n",
    "    torch.as_tensor()\n",
    "\n",
    "The torch.tensor() call is the sort of go-to call, while torch.as_tensor() should be employed when tuning our code for performance. \n",
    " Some things to keep in mind about memory sharing (it works where it can):\n",
    "\n",
    "    1.  Since numpy.ndarray objects are allocated on the CPU, the as_tensor() function must copy the data from the CPU to the GPU when a GPU is being used.\n",
    "    2.  The memory sharing of as_tensor() doesn't work with built-in Python data structures like lists.\n",
    "    3.  The as_tensor() call requires developer knowledge of the sharing feature. This is necessary so we don't inadvertently make an unwanted change in the underlying data without realizing the change impacts multiple objects.\n",
    "    4.  The as_tensor() performance improvement will be greater if there are a lot of back and forth operations between numpy.ndarray objects and tensor objects. However, if there is just a single load operation, there shouldn't be much impact from a performance perspective."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
